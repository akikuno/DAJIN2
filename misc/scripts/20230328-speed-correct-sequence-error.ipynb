{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回の取り組み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `preprocess.correct_sequence_error`が遅いので遅い関数を同定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いつものセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/akihi/Documents/GitHub/DAJIN2\n"
     ]
    }
   ],
   "source": [
    "# ルートディレクトリをPathに含めるおまじない\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "if Path(os.getcwd()).stem != \"DAJIN2\":\n",
    "    parent_path = str(Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.parent)\n",
    "    sys.path.append(parent_path)\n",
    "    os.chdir(parent_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pipの更新\n",
    "pip install -q -U pip\n",
    "pip install -q -U -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stx2でテストする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test-stx2-deletion...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "from src.DAJIN2.core import preprocess, classification, clustering, consensus, report\n",
    "from src.DAJIN2.core.clustering import clustering\n",
    "\n",
    "reload(preprocess)\n",
    "reload(classification)\n",
    "reload(clustering)\n",
    "reload(consensus)\n",
    "reload(report)\n",
    "\n",
    "\n",
    "#### #* 2-cut deletion\n",
    "SAMPLE, CONTROL, ALLELE, NAME, GENOME, DEBUG, THREADS = (\n",
    "    \"examples/del-stx2/barcode25.fq.gz\",\n",
    "    \"examples/del-stx2/barcode30.fq.gz\",\n",
    "    \"examples/del-stx2/design_stx2.fa\",\n",
    "    \"test-stx2-deletion\",\n",
    "    \"mm10\",\n",
    "    True,\n",
    "    14,\n",
    ")\n",
    "\n",
    "print(f\"processing {NAME}...\")\n",
    "\n",
    "##########################################################\n",
    "# Check inputs\n",
    "##########################################################\n",
    "preprocess.check_inputs.check_files(SAMPLE, CONTROL, ALLELE)\n",
    "TEMPDIR = Path(\"DAJINResults\", \".tempdir\", NAME)\n",
    "IS_CACHE_CONTROL = preprocess.check_inputs.exists_cached_control(CONTROL, TEMPDIR)\n",
    "IS_CACHE_GENOME = preprocess.check_inputs.exists_cached_genome(GENOME, TEMPDIR, IS_CACHE_CONTROL)\n",
    "UCSC_URL, GOLDENPATH_URL = None, None\n",
    "if GENOME and not IS_CACHE_GENOME:\n",
    "    UCSC_URL, GOLDENPATH_URL = preprocess.check_inputs.check_and_fetch_genome(GENOME)\n",
    "\n",
    "##########################################################\n",
    "# Format inputs\n",
    "##########################################################\n",
    "SAMPLE_NAME = preprocess.format_inputs.extract_basename(SAMPLE)\n",
    "CONTROL_NAME = preprocess.format_inputs.extract_basename(CONTROL)\n",
    "FASTA_ALLELES = preprocess.format_inputs.dictionize_allele(ALLELE)\n",
    "THREADS = min(THREADS, os.cpu_count()-1)\n",
    "\n",
    "preprocess.format_inputs.make_directories(TEMPDIR, SAMPLE_NAME, CONTROL_NAME)\n",
    "\n",
    "if GENOME:\n",
    "    GENOME_COODINATES = preprocess.format_inputs.fetch_coodinate(GENOME, UCSC_URL, FASTA_ALLELES[\"control\"])\n",
    "    CHROME_SIZE = preprocess.format_inputs.fetch_chrom_size(GENOME_COODINATES[\"chr\"], GENOME, GOLDENPATH_URL)\n",
    "    preprocess.format_inputs.cache_coodinates_and_chromsize(TEMPDIR, GENOME, GENOME_COODINATES, CHROME_SIZE)\n",
    "\n",
    "################################################################################\n",
    "# Export fasta files as single-FASTA format\n",
    "################################################################################\n",
    "# TODO: use yeild, not export\n",
    "for identifier, sequence in FASTA_ALLELES.items():\n",
    "    contents = \"\\n\".join([\">\" + identifier, sequence]) + \"\\n\"\n",
    "    output_fasta = Path(TEMPDIR, \"fasta\", f\"{identifier}.fasta\")\n",
    "    output_fasta.write_text(contents)\n",
    "###############################################################################\n",
    "# Mapping with mappy\n",
    "###############################################################################\n",
    "for path_fasta in Path(TEMPDIR, \"fasta\").glob(\"*.fasta\"):\n",
    "    name_fasta = path_fasta.stem\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, preset=\"splice\", threads=THREADS\n",
    "    )\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, preset=\"splice\", threads=THREADS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# MIDSV conversion\n",
    "########################################################################\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{CONTROL_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{SAMPLE_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Correct CSSPLITS\n",
    "###############################################################################\n",
    "preprocess.correct_sequence_error.execute(TEMPDIR, FASTA_ALLELES, CONTROL_NAME, SAMPLE_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 編集前はThinkpad X1 nanoで**3分41秒**かかりました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import midsv\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def set_indexes(sequence: str):\n",
    "    sequence_length = len(sequence)\n",
    "    num_subset = sequence_length % 5\n",
    "    left_idx = 0\n",
    "    right_idx = sequence_length\n",
    "    if num_subset == 1:\n",
    "        left_idx += 1\n",
    "    elif num_subset == 2:\n",
    "        left_idx += 1\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 3:\n",
    "        left_idx += 2\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 4:\n",
    "        left_idx += 2\n",
    "        right_idx -= 2\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "def count_5mer_indels(cssplits: list[list[str]], left_idx: int, right_idx: int) -> list[dict]:\n",
    "    transposed = [list(t) for t in zip(*cssplits)]\n",
    "    count_5mer = []\n",
    "    for i in range(left_idx, right_idx, 5):\n",
    "        count = {\"ins\": [1] * 5, \"del\": [1] * 5, \"sub\": [1] * 5}\n",
    "        cssplits_5mer = transposed[i : i + 5]\n",
    "        for j, cs in enumerate(cssplits_5mer):\n",
    "            counter = Counter(cs)\n",
    "            for key, cnt in counter.items():\n",
    "                if key.startswith(\"=\") or key == \"N\" or re.search(r\"a|c|g|t|n\", key):\n",
    "                    continue\n",
    "                if key.startswith(\"+\"):\n",
    "                    count[\"ins\"][j] += cnt\n",
    "                elif key.startswith(\"-\"):\n",
    "                    count[\"del\"][j] += cnt\n",
    "                elif key.startswith(\"*\"):\n",
    "                    count[\"sub\"][j] += cnt\n",
    "        count_5mer.append(count)\n",
    "    return count_5mer\n",
    "\n",
    "\n",
    "def remove_minor_indels(cssplits: list[list[str]], count_5mer: list[dict]) -> list[dict]:\n",
    "    coverage = len(cssplits)\n",
    "    count_5mer_filtered = []\n",
    "    for count in count_5mer:\n",
    "        dict_mutation = defaultdict(list)\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            if all(True if c < coverage * 0.01 else False for c in count[mutation]):\n",
    "                dict_mutation[mutation] = [1] * 5\n",
    "            else:\n",
    "                dict_mutation[mutation] = count[mutation]\n",
    "        count_5mer_filtered.append(dict_mutation)\n",
    "    return count_5mer_filtered\n",
    "\n",
    "\n",
    "def extract_sequence_errors(count_5mer_sample, count_5mer_control):\n",
    "    sequence_errors = [set() for _ in range(len(count_5mer_sample))]\n",
    "    dists = defaultdict(list)\n",
    "    # Calculate Jensen-Shannon distance\n",
    "    for samp, cont in zip(count_5mer_sample, count_5mer_control):\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            s = samp[mutation]\n",
    "            c = cont[mutation]\n",
    "            dists[mutation].append(distance.jensenshannon(s, c))\n",
    "    # Discrimitate seq errors and real mutation using Hotelling's T-squared distribution\n",
    "    dists_all = np.array(list(dists.values())).flatten()\n",
    "    avg = np.average(dists_all[~np.isnan(dists_all)])\n",
    "    var = np.var(dists_all[~np.isnan(dists_all)])\n",
    "    threshold = 0.05\n",
    "    for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "        dists_subset = dists[mutation]\n",
    "        scores = [(xi - avg) ** 2 / var for xi in dists_subset]\n",
    "        thres = stats.chi2.interval(1 - threshold, 1)[1]\n",
    "        for i, score in enumerate(scores):\n",
    "            # 'nan' means the two distributions have too different, so it could be a real mutation\n",
    "            if np.isnan(score):\n",
    "                continue\n",
    "            if score < thres:\n",
    "                sequence_errors[i].add(mutation)\n",
    "    return sequence_errors\n",
    "\n",
    "\n",
    "def replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx):\n",
    "    cssplits_replaced = []\n",
    "    for samp in cssplits_sample:\n",
    "        samp_replaced = deepcopy(samp)\n",
    "        for idx_error, idx_5mer in enumerate(range(left_idx, right_idx, 5)):\n",
    "            samp_5mer = samp[idx_5mer : idx_5mer + 5]\n",
    "            error = sequence_errors[idx_error]\n",
    "            if \"ins\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"+\") else cs for cs in samp_5mer]\n",
    "            if \"del\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"-\") else cs for cs in samp_5mer]\n",
    "            if \"sub\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"*\") else cs for cs in samp_5mer]\n",
    "            samp_replaced[idx_5mer : idx_5mer + 5] = samp_5mer\n",
    "        cssplits_replaced.append(samp_replaced)\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "def replace_atmark(cssplits: list[list[str]], sequence: str) -> list[list[str]]:\n",
    "    random.seed(1)\n",
    "    cssplits_replaced = deepcopy(cssplits)\n",
    "    sequence_length = len(sequence)\n",
    "    for i in range(1, sequence_length - 1):\n",
    "        cssplits_atmark = defaultdict(str)\n",
    "        cssplits_sampling_key = defaultdict(list)\n",
    "        cssplits_sampling_all = []\n",
    "        flag_all_atmark = True\n",
    "        for idx, cssplit in enumerate(cssplits):\n",
    "            key = \",\".join([cssplit[i - 1], cssplit[i + 1]])\n",
    "            if cssplit[i] == \"@\":\n",
    "                cssplits_atmark[idx] = key\n",
    "            else:\n",
    "                cssplits_sampling_key[key].append(cssplit[i])\n",
    "                cssplits_sampling_all.append(cssplit[i])\n",
    "                flag_all_atmark = False\n",
    "        for idx, key in cssplits_atmark.items():\n",
    "            if flag_all_atmark:\n",
    "                cssplits_replaced[idx][i] = \"N\"\n",
    "            elif cssplits_sampling_key[key]:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_key[key])\n",
    "            else:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_all)\n",
    "    for cs in cssplits_replaced:\n",
    "        if cs[0] == \"@\":\n",
    "            cs[0] = \"N\"\n",
    "        if cs[-1] == \"@\":\n",
    "            cs[-1] = \"N\"\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# main\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def execute(TEMPDIR: Path, FASTA_ALLELES: dict[str, str], CONTROL_NAME: str, SAMPLE_NAME: str) -> None:\n",
    "    for allele, sequence in FASTA_ALLELES.items():\n",
    "        midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "        midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "        cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "        cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]\n",
    "        # Extract sequence errors\n",
    "        left_idx, right_idx = set_indexes(sequence)\n",
    "        count_5mer_sample = count_5mer_indels(cssplits_sample, left_idx, right_idx)\n",
    "        count_5mer_control = count_5mer_indels(cssplits_control, left_idx, right_idx)\n",
    "        count_5mer_sample = remove_minor_indels(cssplits_sample, count_5mer_sample)\n",
    "        count_5mer_control = remove_minor_indels(cssplits_control, count_5mer_control)\n",
    "        sequence_errors = extract_sequence_errors(count_5mer_sample, count_5mer_control)\n",
    "        # Correct sequence errors\n",
    "        cssplits_sample_error_replaced = replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx)\n",
    "        cssplits_control_error_replaced = replace_errors_to_atmark(\n",
    "            cssplits_control, sequence_errors, left_idx, right_idx\n",
    "        )\n",
    "        cssplits_sample_atmark_replaced = replace_atmark(cssplits_sample_error_replaced, sequence)\n",
    "        cssplits_control_atmark_replaced = replace_atmark(cssplits_control_error_replaced, sequence)\n",
    "        # Replace CSSPLIT\n",
    "        cssplits_sample_corrected = [\",\".join(cs) for cs in cssplits_sample_atmark_replaced]\n",
    "        cssplits_control_corrected = [\",\".join(cs) for cs in cssplits_control_atmark_replaced]\n",
    "        for i, cssplits in enumerate(cssplits_sample_corrected):\n",
    "            midsv_sample[i][\"CSSPLIT\"] = cssplits\n",
    "        for i, cssplits in enumerate(cssplits_control_corrected):\n",
    "            midsv_control[i][\"CSSPLIT\"] = cssplits\n",
    "        midsv.write_jsonl(midsv_sample, Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\"))\n",
    "        midsv.write_jsonl(midsv_control, Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# MIDSV conversion\n",
    "########################################################################\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{CONTROL_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{SAMPLE_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele=\"control\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### どの関数が時間がかかるのかチェックします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence errors\n",
    "left_idx, right_idx = set_indexes(sequence)\n",
    "count_5mer_sample = count_5mer_indels(cssplits_sample, left_idx, right_idx)\n",
    "count_5mer_control = count_5mer_indels(cssplits_control, left_idx, right_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_5mer_sample = remove_minor_indels(cssplits_sample, count_5mer_sample)\n",
    "count_5mer_control = remove_minor_indels(cssplits_control, count_5mer_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_errors = extract_sequence_errors(count_5mer_sample, count_5mer_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct sequence errors\n",
    "cssplits_sample_error_replaced = replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cssplits_control_error_replaced = replace_errors_to_atmark(\n",
    "    cssplits_control, sequence_errors, left_idx, right_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cssplits_sample_atmark_replaced = replace_atmark(cssplits_sample_error_replaced, sequence)\n",
    "cssplits_control_atmark_replaced = replace_atmark(cssplits_control_error_replaced, sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace CSSPLIT\n",
    "cssplits_sample_corrected = [\",\".join(cs) for cs in cssplits_sample_atmark_replaced]\n",
    "cssplits_control_corrected = [\",\".join(cs) for cs in cssplits_control_atmark_replaced]\n",
    "for i, cssplits in enumerate(cssplits_sample_corrected):\n",
    "    midsv_sample[i][\"CSSPLIT\"] = cssplits\n",
    "for i, cssplits in enumerate(cssplits_control_corrected):\n",
    "    midsv_control[i][\"CSSPLIT\"] = cssplits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とくに遅いのは以下の2つ\n",
    "- `replace_errors_to_atmark`\n",
    "- `replace_atmark`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `replace_errors_to_atmark`\n",
    "- for内部でdeepcopyしていると遅くなりそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx):\n",
    "    cssplits_replaced = []\n",
    "    for samp in cssplits_sample:\n",
    "        samp_replaced = deepcopy(samp)\n",
    "        for idx_error, idx_5mer in enumerate(range(left_idx, right_idx, 5)):\n",
    "            samp_5mer = samp[idx_5mer : idx_5mer + 5]\n",
    "            error = sequence_errors[idx_error]\n",
    "            if \"ins\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"+\") else cs for cs in samp_5mer]\n",
    "            if \"del\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"-\") else cs for cs in samp_5mer]\n",
    "            if \"sub\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"*\") else cs for cs in samp_5mer]\n",
    "            samp_replaced[idx_5mer : idx_5mer + 5] = samp_5mer\n",
    "        cssplits_replaced.append(samp_replaced)\n",
    "    return cssplits_replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx):\n",
    "    cssplits_replaced = []\n",
    "    for samp in cssplits_sample:\n",
    "        for idx_error, idx_5mer in enumerate(range(left_idx, right_idx, 5)):\n",
    "            samp_5mer = samp[idx_5mer : idx_5mer + 5]\n",
    "            error = sequence_errors[idx_error]\n",
    "            if \"ins\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"+\") else cs for cs in samp_5mer]\n",
    "            if \"del\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"-\") else cs for cs in samp_5mer]\n",
    "            if \"sub\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"*\") else cs for cs in samp_5mer]\n",
    "            samp[idx_5mer : idx_5mer + 5] = samp_5mer\n",
    "        cssplits_replaced.append(samp)\n",
    "    return cssplits_replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = before_replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = after_replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert before == after"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `deepcopy`をfor文の外にして一度だけの実行にすることで3秒ほど短縮しました\n",
    "- いっぽうで一度の`deepcopy`にかかる時間が6秒くらいで相当かかっていることが気になりました\n",
    "    - `deepcopy`を無くすことで**19秒から6秒**への短縮に成功しました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `replace_atmark`\n",
    "- deepcopyを外す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_replace_atmark(cssplits: list[list[str]], sequence: str) -> list[list[str]]:\n",
    "    random.seed(1)\n",
    "    cssplits_replaced = deepcopy(cssplits)\n",
    "    sequence_length = len(sequence)\n",
    "    for i in range(1, sequence_length - 1):\n",
    "        cssplits_atmark = defaultdict(str)\n",
    "        cssplits_sampling_key = defaultdict(list)\n",
    "        cssplits_sampling_all = []\n",
    "        flag_all_atmark = True\n",
    "        for idx, cssplit in enumerate(cssplits):\n",
    "            key = \",\".join([cssplit[i - 1], cssplit[i + 1]])\n",
    "            if cssplit[i] == \"@\":\n",
    "                cssplits_atmark[idx] = key\n",
    "            else:\n",
    "                cssplits_sampling_key[key].append(cssplit[i])\n",
    "                cssplits_sampling_all.append(cssplit[i])\n",
    "                flag_all_atmark = False\n",
    "        for idx, key in cssplits_atmark.items():\n",
    "            if flag_all_atmark:\n",
    "                cssplits_replaced[idx][i] = \"N\"\n",
    "            elif cssplits_sampling_key[key]:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_key[key])\n",
    "            else:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_all)\n",
    "    for cs in cssplits_replaced:\n",
    "        if cs[0] == \"@\":\n",
    "            cs[0] = \"N\"\n",
    "        if cs[-1] == \"@\":\n",
    "            cs[-1] = \"N\"\n",
    "    return cssplits_replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_replace_atmark(cssplits: list[list[str]], sequence: str) -> list[list[str]]:\n",
    "    random.seed(1)\n",
    "    cssplits_replaced = cssplits.copy()\n",
    "    sequence_length = len(sequence)\n",
    "    for i in range(1, sequence_length - 1):\n",
    "        cssplits_atmark = defaultdict(str)\n",
    "        cssplits_sampling_key = defaultdict(list)\n",
    "        cssplits_sampling_all = []\n",
    "        flag_all_atmark = True\n",
    "        for idx, cssplit in enumerate(cssplits):\n",
    "            key = \",\".join([cssplit[i - 1], cssplit[i + 1]])\n",
    "            if cssplit[i] == \"@\":\n",
    "                cssplits_atmark[idx] = key\n",
    "            else:\n",
    "                cssplits_sampling_key[key].append(cssplit[i])\n",
    "                cssplits_sampling_all.append(cssplit[i])\n",
    "                flag_all_atmark = False\n",
    "        for idx, key in cssplits_atmark.items():\n",
    "            if flag_all_atmark:\n",
    "                cssplits_replaced[idx][i] = \"N\"\n",
    "            elif cssplits_sampling_key[key]:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_key[key])\n",
    "            else:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_all)\n",
    "    for cs in cssplits_replaced:\n",
    "        if cs[0] == \"@\":\n",
    "            cs[0] = \"N\"\n",
    "        if cs[-1] == \"@\":\n",
    "            cs[-1] = \"N\"\n",
    "    return cssplits_replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_replace_atmark(cssplits_sample_error_replaced, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_replace_atmark(cssplits_sample_error_replaced, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4327\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `replaceNtoD`を組み込みました"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に取り組むこと"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "+ [ ] Insertionのなかにある変異を同定する手法を考案する\n",
    "+ [ ] Ayabe-taks1のright_loxpがいまいちな理由を考察する\n",
    "+ [ ] `preprocess.correct_sequence_error`の処理速度の改善\n",
    "+ [ ] `preprocess.correct_sequence_error.replace_atmark`のコードがわかりにくい\n",
    "    + テストを用意してリファクタリングする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "\n",
    "両者については、`correct_sequence_error`などで補正するときに、**変異候補の塩基配列のみを対象とする**ことで対応できる可能性がある\n",
    "\n",
    "- 変異候補の塩基配列のみを対象とする\n",
    "    - 両端が欠失しているようなリードについて、変異候補部位を含まないリードは`uncategorized`といったカテゴリにできる\n",
    "    - よって**変異候補部位を含むか含まないか**を考えることで、短いリードや両端が欠失しているリードの分類が可能になる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a09285d6cbc768c1977f96e8deb5ca1ec0d08675e9573ed6dfd37fd7d91de663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
