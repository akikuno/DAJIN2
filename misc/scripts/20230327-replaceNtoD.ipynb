{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回の取り組み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 両端の`N`とそれ以外の`N`を一緒に考えると難しくなるので、これらを区別する\n",
    "- 両端から連続する`N`以外の`N`を欠失として変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いつものセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Research/DAJIN2\n"
     ]
    }
   ],
   "source": [
    "# ルートディレクトリをPathに含めるおまじない\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "if Path(os.getcwd()).stem != \"DAJIN2\":\n",
    "    parent_path = str(Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.parent)\n",
    "    sys.path.append(parent_path)\n",
    "    os.chdir(parent_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pipの更新\n",
    "pip install -q -U pip\n",
    "pip install -q -U -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `preprocess.replaceNtoD.py`を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test-stx2-deletion...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "from src.DAJIN2.core import preprocess, classification, clustering, consensus, report\n",
    "from src.DAJIN2.core.clustering import clustering\n",
    "\n",
    "reload(preprocess)\n",
    "reload(classification)\n",
    "reload(clustering)\n",
    "reload(consensus)\n",
    "reload(report)\n",
    "\n",
    "\n",
    "#### #* 2-cut deletion\n",
    "SAMPLE, CONTROL, ALLELE, NAME, GENOME, DEBUG, THREADS = (\n",
    "    \"examples/del-stx2/barcode25.fq.gz\",\n",
    "    \"examples/del-stx2/barcode30.fq.gz\",\n",
    "    \"examples/del-stx2/design_stx2.fa\",\n",
    "    \"test-stx2-deletion\",\n",
    "    \"mm10\",\n",
    "    True,\n",
    "    14,\n",
    ")\n",
    "\n",
    "print(f\"processing {NAME}...\")\n",
    "\n",
    "##########################################################\n",
    "# Check inputs\n",
    "##########################################################\n",
    "preprocess.check_inputs.check_files(SAMPLE, CONTROL, ALLELE)\n",
    "TEMPDIR = Path(\"DAJINResults\", \".tempdir\", NAME)\n",
    "IS_CACHE_CONTROL = preprocess.check_inputs.exists_cached_control(CONTROL, TEMPDIR)\n",
    "IS_CACHE_GENOME = preprocess.check_inputs.exists_cached_genome(GENOME, TEMPDIR, IS_CACHE_CONTROL)\n",
    "UCSC_URL, GOLDENPATH_URL = None, None\n",
    "if GENOME and not IS_CACHE_GENOME:\n",
    "    UCSC_URL, GOLDENPATH_URL = preprocess.check_inputs.check_and_fetch_genome(GENOME)\n",
    "\n",
    "##########################################################\n",
    "# Format inputs\n",
    "##########################################################\n",
    "SAMPLE_NAME = preprocess.format_inputs.extract_basename(SAMPLE)\n",
    "CONTROL_NAME = preprocess.format_inputs.extract_basename(CONTROL)\n",
    "FASTA_ALLELES = preprocess.format_inputs.dictionize_allele(ALLELE)\n",
    "THREADS = min(THREADS, os.cpu_count()-1)\n",
    "\n",
    "preprocess.format_inputs.make_directories(TEMPDIR, SAMPLE_NAME, CONTROL_NAME)\n",
    "\n",
    "if GENOME:\n",
    "    GENOME_COODINATES = preprocess.format_inputs.fetch_coodinate(GENOME, UCSC_URL, FASTA_ALLELES[\"control\"])\n",
    "    CHROME_SIZE = preprocess.format_inputs.fetch_chrom_size(GENOME_COODINATES[\"chr\"], GENOME, GOLDENPATH_URL)\n",
    "    preprocess.format_inputs.cache_coodinates_and_chromsize(TEMPDIR, GENOME, GENOME_COODINATES, CHROME_SIZE)\n",
    "\n",
    "# ################################################################################\n",
    "# # Export fasta files as single-FASTA format\n",
    "# ################################################################################\n",
    "# # TODO: use yeild, not export\n",
    "# for identifier, sequence in FASTA_ALLELES.items():\n",
    "#     contents = \"\\n\".join([\">\" + identifier, sequence]) + \"\\n\"\n",
    "#     output_fasta = Path(TEMPDIR, \"fasta\", f\"{identifier}.fasta\")\n",
    "#     output_fasta.write_text(contents)\n",
    "# ###############################################################################\n",
    "# # Mapping with mappy\n",
    "# ###############################################################################\n",
    "# for path_fasta in Path(TEMPDIR, \"fasta\").glob(\"*.fasta\"):\n",
    "#     name_fasta = path_fasta.stem\n",
    "#     preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, threads=THREADS)\n",
    "#     preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, threads=THREADS)\n",
    "#     preprocess.mappy_align.output_sam(\n",
    "#         TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, preset=\"splice\", threads=THREADS\n",
    "#     )\n",
    "#     preprocess.mappy_align.output_sam(\n",
    "#         TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, preset=\"splice\", threads=THREADS\n",
    "#     )\n",
    "# ########################################################################\n",
    "# # MIDSV conversion\n",
    "# ########################################################################\n",
    "# for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{CONTROL_NAME}_splice_*\"):\n",
    "#     preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "# for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{SAMPLE_NAME}_splice_*\"):\n",
    "#     preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "# ###############################################################################\n",
    "# # Correct CSSPLITS\n",
    "# ###############################################################################\n",
    "# # preprocess.correct_revititive_deletions.execute(TEMPDIR, FASTA_ALLELES, CONTROL_NAME, SAMPLE_NAME)\n",
    "# preprocess.correct_sequence_error.execute(TEMPDIR, FASTA_ALLELES, CONTROL_NAME, SAMPLE_NAME)\n",
    "# preprocess.correct_knockin.execute(TEMPDIR, FASTA_ALLELES, CONTROL_NAME, SAMPLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import midsv\n",
    "\n",
    "def transpose(cssplits):\n",
    "    return [list(cs) for cs in zip(*cssplits)]\n",
    "\n",
    "\n",
    "def replaceNtoD(cssplits_sample, sequence):\n",
    "    cssplits_replaced = deepcopy(cssplits_sample)\n",
    "    for i, cssplits in enumerate(cssplits_sample):\n",
    "        flag_n_start = True\n",
    "        flag_n_end = True\n",
    "        for j, (start, end) in enumerate(zip(cssplits, cssplits[::-1])):\n",
    "            if j == (len(cssplits)+1) // 2:\n",
    "                break\n",
    "            if flag_n_start and start != \"N\":\n",
    "                    flag_n_start = False\n",
    "            if flag_n_end and end != \"N\":\n",
    "                    flag_n_end = False\n",
    "            if not flag_n_start and start == \"N\":\n",
    "                cssplits_replaced[i][j] = f\"-{sequence[j]}\"\n",
    "            if not flag_n_end and end == \"N\":\n",
    "                j_inv = len(cssplits) - j - 1\n",
    "                cssplits_replaced[i][j_inv] = f\"-{sequence[j_inv]}\"\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# main\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def execute(TEMPDIR: Path, FASTA_ALLELES: dict, SAMPLE_NAME: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert any `N` as deletions other than consecutive `N` from both ends\n",
    "    \"\"\"\n",
    "    for allele, sequence in FASTA_ALLELES.items():\n",
    "        midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "        cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "        cssplits_replaced = replaceNtoD(cssplits_sample, sequence)\n",
    "        midsv_cssplits = [\",\".join(cs) for cs in cssplits_replaced]\n",
    "        # Save as a json\n",
    "        for i, cssplits in enumerate(midsv_cssplits):\n",
    "            midsv_sample[i][\"CSSPLIT\"] = cssplits\n",
    "        midsv.write_jsonl(midsv_sample, Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cssplits_sample = [\n",
    "    [\"N\", \"N\", \"N\", \"=A\", \"N\" , \"=C\", \"N\", \"N\"],\n",
    "    [\"N\", \"N\", \"=A\", \"N\", \"N\" , \"=C\", \"=C\", \"N\"],\n",
    "            ]\n",
    "sequence = \"GCAACCCC\"\n",
    "# print(len(cssplits))\n",
    "# print(len(cssplits) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cssplits_replaced = replaceNtoD(cssplits_sample, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N', 'N', 'N', '=A', 'N', '=C', 'N', 'N'], ['N', 'N', '=A', 'N', 'N', '=C', '=C', 'N']]\n",
      "[['N', 'N', 'N', '=A', '-C', '=C', 'N', 'N'], ['N', 'N', '=A', '-A', '-C', '=C', '=C', 'N']]\n"
     ]
    }
   ],
   "source": [
    "print(cssplits_sample)\n",
    "print(cssplits_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STX2大型欠失アレルを用いてreplaceNtoDの動作を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls DAJINResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Export fasta files as single-FASTA format\n",
    "################################################################################\n",
    "# TODO: use yeild, not export\n",
    "for identifier, sequence in FASTA_ALLELES.items():\n",
    "    contents = \"\\n\".join([\">\" + identifier, sequence]) + \"\\n\"\n",
    "    output_fasta = Path(TEMPDIR, \"fasta\", f\"{identifier}.fasta\")\n",
    "    output_fasta.write_text(contents)\n",
    "###############################################################################\n",
    "# Mapping with mappy\n",
    "###############################################################################\n",
    "for path_fasta in Path(TEMPDIR, \"fasta\").glob(\"*.fasta\"):\n",
    "    name_fasta = path_fasta.stem\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, preset=\"splice\", threads=THREADS\n",
    "    )\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, preset=\"splice\", threads=THREADS\n",
    "    )\n",
    "########################################################################\n",
    "# MIDSV conversion\n",
    "########################################################################\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{CONTROL_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{SAMPLE_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "query=\"DAJINResults/.tempdir/test-stx2-deletion/sam/barcode25_splice_control.sam\"\n",
    "samtools sort \"$query\" > tmp.bam\n",
    "samtools index tmp.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=T', '=G', '=G', '=G', '=G', '=T', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "large_del = \"a21d42f44f56\"\n",
    "import midsv\n",
    "midsv_sample=midsv.read_jsonl(\"DAJINResults/.tempdir/test-stx2-deletion/midsv/barcode25_splice_control.jsonl\")\n",
    "for m in midsv_sample:\n",
    "    if large_del in m[\"QNAME\"]:\n",
    "        break\n",
    "print(m[\"CSSPLIT\"].split(\",\")[1310:1350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele=\"control\"\n",
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "cssplits_replaced = replaceNtoD(cssplits_sample, sequence)\n",
    "midsv_cssplits = [\",\".join(cs) for cs in cssplits_replaced]\n",
    "# Save as a json\n",
    "for i, cssplits in enumerate(midsv_cssplits):\n",
    "    midsv_sample[i][\"CSSPLIT\"] = cssplits\n",
    "midsv.write_jsonl(midsv_sample, Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', '=C', '=C']\n",
      "['=T', '=G', '=G', '=G', '=G', '=T', '-G', '-T', '-G', '-G', '-A', '-A', '-G', '-T', '-T', '-G', '-G', '-T', '-A', '-C', '-C', '-T', '-T', '-T', '-T', '-T', '-C', '-C', '-T', '-T', '-A', '-G', '-A', '-G', '-T', '-T', '-T', '-T', '-A', '-T']\n",
      "['=T', '=G', '=T', '=G', '=T']\n"
     ]
    }
   ],
   "source": [
    "large_del = \"a21d42f44f56\"\n",
    "import midsv\n",
    "midsv_sample=midsv.read_jsonl(\"DAJINResults/.tempdir/test-stx2-deletion/midsv/barcode25_splice_control.jsonl\")\n",
    "for m in midsv_sample:\n",
    "    if large_del in m[\"QNAME\"]:\n",
    "        break\n",
    "print(m[\"CSSPLIT\"].split(\",\")[0:5])\n",
    "print(m[\"CSSPLIT\"].split(\",\")[1310:1350])\n",
    "print(m[\"CSSPLIT\"].split(\",\")[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=T', '=G', '=T', '=G', 'N']\n",
      "['=T', '*GA', '=G', '=G', '=G', '=T', '-G', '-T', '-G', '-G', '-A', '-A', '-G', '-T', '-T', '-G', '-G', '-T', '-A', '-C']\n",
      "['N', 'N', 'N', 'N', 'N']\n",
      "['=T', '-G', '=G', '=G', '=G', '=T', '=G', '=T', '=G', '-G', '-A', '=A', '=G', '=T', '=T', '=G', '=G', '=T', '=A', '=C']\n",
      "['N', 'N', 'N', 'N', 'N']\n",
      "['-T', '-G', '-G', '-G', '-G', '-T', '-G', '-T', '-G', '-G', '-A', '-A', '-G', '-T', '-T', '-G', '-G', '-T', '-A', '-C']\n",
      "['N', 'N', 'N', 'N', 'N']\n",
      "['=T', '=G', '=G', '=G', '=G', '=T', '=G', '=T', '=G', '+G|+A|=G', '=A', '=A', '=G', '=T', '=T', '-G', '=G', '*TA', '=A', '=C']\n",
      "['N', 'N', 'N', 'N', 'N']\n",
      "['=T', '=G', '=G', '=G', '=G', '=T', '=G', '=T', '=G', '=G', '=A', '=A', '=G', '=T', '=T', '=G', '=G', '=T', '=A', '=C']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for m in midsv_sample:\n",
    "    if m[\"CSSPLIT\"].endswith(\"N\"):\n",
    "        print(m[\"CSSPLIT\"].split(\",\")[-5:])\n",
    "        print(m[\"CSSPLIT\"].split(\",\")[1310:1330])\n",
    "        count += 1\n",
    "    if count == 5:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MIDSVの更新をして、最終的にリファレンス配列と長さが異なる配列を除去することにしました"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に取り組むこと"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "+ [ ] Insertionのなかにある変異を同定する手法を考案する\n",
    "+ [ ] Ayabe-taks1のright_loxpがいまいちな理由を考察する\n",
    "+ [ ] `preprocess.correct_sequence_error.replace_atmark`のコードがわかりにくい\n",
    "    + テストを用意してリファクタリングする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "\n",
    "両者については、`correct_sequence_error`などで補正するときに、**変異候補の塩基配列のみを対象とする**ことで対応できる可能性がある\n",
    "\n",
    "- 変異候補の塩基配列のみを対象とする\n",
    "    - 両端が欠失しているようなリードについて、変異候補部位を含まないリードは`uncategorized`といったカテゴリにできる\n",
    "    - よって**変異候補部位を含むか含まないか**を考えることで、短いリードや両端が欠失しているリードの分類が可能になる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a09285d6cbc768c1977f96e8deb5ca1ec0d08675e9573ed6dfd37fd7d91de663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
