{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回の取り組み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記の2つについて、`correct_sequence_error`などで補正するときに、**変異候補の塩基配列のみを対象とする**ことで対応できる可能性がある\n",
    "\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "\n",
    "- 変異候補の塩基配列のみを対象とする\n",
    "    - 両端が欠失しているようなリードについて、変異候補部位を含まないリードは`uncategorized`といったカテゴリにできる\n",
    "    - よって**変異候補部位を含むか含まないか**を考えることで、短いリードや両端が欠失しているリードの分類が可能になる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いつものセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/akihi/Documents/GitHub/DAJIN2\n"
     ]
    }
   ],
   "source": [
    "# ルートディレクトリをPathに含めるおまじない\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "if Path(os.getcwd()).stem != \"DAJIN2\":\n",
    "    parent_path = str(Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.parent)\n",
    "    sys.path.append(parent_path)\n",
    "    os.chdir(parent_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pipの更新\n",
    "pip install -q -U pip\n",
    "pip install -q -U -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `preprocess.correct_sequence_error`と`preprocess.correct_knockin`でシークエンスエラーではない変異箇所を抽出する\n",
    "- もしくは、コードを独立させたほうが良いかもしれません\n",
    "    - correctionが終わったあとに、sampleとcontrolと比べて変異のある塩基位置を抽出する\n",
    "    - correctionと独立させることで、correctionの方法が変わっても塩基位置を抽出するコードを変える必要がなくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test-pm-tyr...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "from src.DAJIN2.core import preprocess, classification, clustering, consensus, report\n",
    "from src.DAJIN2.core.clustering import clustering\n",
    "\n",
    "reload(preprocess)\n",
    "reload(classification)\n",
    "reload(clustering)\n",
    "reload(consensus)\n",
    "reload(report)\n",
    "\n",
    "\n",
    "##### # * Point mutation\n",
    "SAMPLE, CONTROL, ALLELE, NAME, GENOME, DEBUG, THREADS = (\n",
    "    \"examples/pm-tyr/barcode31.fq.gz\",\n",
    "    \"examples/pm-tyr/barcode32.fq.gz\",\n",
    "    \"examples/pm-tyr/design_tyr.fa\",\n",
    "    \"test-pm-tyr\",\n",
    "    \"mm10\",\n",
    "    True,\n",
    "    14,\n",
    ")\n",
    "\n",
    "print(f\"processing {NAME}...\")\n",
    "\n",
    "##########################################################\n",
    "# Check inputs\n",
    "##########################################################\n",
    "preprocess.check_inputs.check_files(SAMPLE, CONTROL, ALLELE)\n",
    "TEMPDIR = Path(\"DAJINResults\", \".tempdir\", NAME)\n",
    "IS_CACHE_CONTROL = preprocess.check_inputs.exists_cached_control(CONTROL, TEMPDIR)\n",
    "IS_CACHE_GENOME = preprocess.check_inputs.exists_cached_genome(GENOME, TEMPDIR, IS_CACHE_CONTROL)\n",
    "UCSC_URL, GOLDENPATH_URL = None, None\n",
    "if GENOME and not IS_CACHE_GENOME:\n",
    "    UCSC_URL, GOLDENPATH_URL = preprocess.check_inputs.check_and_fetch_genome(GENOME)\n",
    "\n",
    "##########################################################\n",
    "# Format inputs\n",
    "##########################################################\n",
    "SAMPLE_NAME = preprocess.format_inputs.extract_basename(SAMPLE)\n",
    "CONTROL_NAME = preprocess.format_inputs.extract_basename(CONTROL)\n",
    "FASTA_ALLELES = preprocess.format_inputs.dictionize_allele(ALLELE)\n",
    "THREADS = min(THREADS, os.cpu_count()-1)\n",
    "\n",
    "preprocess.format_inputs.make_directories(TEMPDIR, SAMPLE_NAME, CONTROL_NAME)\n",
    "\n",
    "if GENOME:\n",
    "    GENOME_COODINATES = preprocess.format_inputs.fetch_coodinate(GENOME, UCSC_URL, FASTA_ALLELES[\"control\"])\n",
    "    CHROME_SIZE = preprocess.format_inputs.fetch_chrom_size(GENOME_COODINATES[\"chr\"], GENOME, GOLDENPATH_URL)\n",
    "    preprocess.format_inputs.cache_coodinates_and_chromsize(TEMPDIR, GENOME, GENOME_COODINATES, CHROME_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midsv\n",
    "from collections import Counter\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import midsv\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def set_indexes(sequence: str):\n",
    "    sequence_length = len(sequence)\n",
    "    num_subset = sequence_length % 5\n",
    "    left_idx = 0\n",
    "    right_idx = sequence_length\n",
    "    if num_subset == 1:\n",
    "        left_idx += 1\n",
    "    elif num_subset == 2:\n",
    "        left_idx += 1\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 3:\n",
    "        left_idx += 2\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 4:\n",
    "        left_idx += 2\n",
    "        right_idx -= 2\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "def count_indels_5mer(cssplits: list[list[str]], left_idx: int, right_idx: int) -> list[dict]:\n",
    "    transposed = [list(t) for t in zip(*cssplits)]\n",
    "    count_indels_5mer = []\n",
    "    for i in range(left_idx, right_idx, 5):\n",
    "        count = {\"ins\": [1] * 5, \"del\": [1] * 5, \"sub\": [1] * 5}\n",
    "        cssplits_5mer = transposed[i : i + 5]\n",
    "        for j, cs in enumerate(cssplits_5mer):\n",
    "            counter = Counter(cs)\n",
    "            for key, cnt in counter.items():\n",
    "                if key.startswith(\"=\") or key == \"N\" or re.search(r\"a|c|g|t|n\", key):\n",
    "                    continue\n",
    "                if key.startswith(\"+\"):\n",
    "                    count[\"ins\"][j] += cnt\n",
    "                elif key.startswith(\"-\"):\n",
    "                    count[\"del\"][j] += cnt\n",
    "                elif key.startswith(\"*\"):\n",
    "                    count[\"sub\"][j] += cnt\n",
    "        count_indels_5mer.append(count)\n",
    "    return count_indels_5mer\n",
    "\n",
    "\n",
    "def extract_sequence_errors(count_5mer_sample, count_5mer_control):\n",
    "    sequence_errors = [set() for _ in range(len(count_5mer_sample))]\n",
    "    dists = defaultdict(list)\n",
    "    # Calculate Jensen-Shannon distance\n",
    "    for samp, cont in zip(count_5mer_sample, count_5mer_control):\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            s = samp[mutation]\n",
    "            c = cont[mutation]\n",
    "            dists[mutation].append(distance.jensenshannon(s, c))\n",
    "    # Discrimitate seq errors and real mutation using Hotelling's T-squared distribution\n",
    "    dists_all = np.array(list(dists.values())).flatten()\n",
    "    avg = np.average(dists_all[~np.isnan(dists_all)])\n",
    "    var = np.var(dists_all[~np.isnan(dists_all)])\n",
    "    threshold = 0.05\n",
    "    for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "        dists_subset = dists[mutation]\n",
    "        scores = [(xi - avg) ** 2 / var for xi in dists_subset]\n",
    "        thres = stats.chi2.interval(1 - threshold, 1)[1]\n",
    "        for i, score in enumerate(scores):\n",
    "            # 'nan' means the two distributions have too different, so it could be a real mutation\n",
    "            if np.isnan(score):\n",
    "                continue\n",
    "            if score < thres:\n",
    "                sequence_errors[i].add(mutation)\n",
    "    return sequence_errors\n",
    "\n",
    "\n",
    "def replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx):\n",
    "    cssplits_replaced = []\n",
    "    for samp in cssplits_sample:\n",
    "        samp_replaced = deepcopy(samp)\n",
    "        for idx_error, idx_5mer in enumerate(range(left_idx, right_idx, 5)):\n",
    "            samp_5mer = samp[idx_5mer : idx_5mer + 5]\n",
    "            error = sequence_errors[idx_error]\n",
    "            if \"ins\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"+\") else cs for cs in samp_5mer]\n",
    "            if \"del\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"-\") else cs for cs in samp_5mer]\n",
    "            if \"sub\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"*\") else cs for cs in samp_5mer]\n",
    "            samp_replaced[idx_5mer : idx_5mer + 5] = samp_5mer\n",
    "        cssplits_replaced.append(samp_replaced)\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "def replace_atmark(cssplits: list[list[str]], sequence: str) -> list[list[str]]:\n",
    "    random.seed(1)\n",
    "    cssplits_replaced = deepcopy(cssplits)\n",
    "    sequence_length = len(sequence)\n",
    "    for i in range(1, sequence_length - 1):\n",
    "        cssplits_atmark = defaultdict(str)\n",
    "        cssplits_sampling_key = defaultdict(list)\n",
    "        cssplits_sampling_all = []\n",
    "        flag_all_atmark = True\n",
    "        for idx, cssplit in enumerate(cssplits):\n",
    "            key = \",\".join([cssplit[i - 1], cssplit[i + 1]])\n",
    "            if cssplit[i] == \"@\":\n",
    "                cssplits_atmark[idx] = key\n",
    "            else:\n",
    "                cssplits_sampling_key[key].append(cssplit[i])\n",
    "                cssplits_sampling_all.append(cssplit[i])\n",
    "                flag_all_atmark = False\n",
    "        for idx, key in cssplits_atmark.items():\n",
    "            if flag_all_atmark:\n",
    "                cssplits_replaced[idx][i] = \"N\"\n",
    "            elif cssplits_sampling_key[key]:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_key[key])\n",
    "            else:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_all)\n",
    "    for cs in cssplits_replaced:\n",
    "        if cs[0] == \"@\":\n",
    "            cs[0] = \"=\" + sequence[0]\n",
    "        if cs[-1] == \"@\":\n",
    "            cs[-1] = \"=\" + sequence[-1]\n",
    "    return cssplits_replaced\n",
    "\n",
    "def transpose(cssplits):\n",
    "    return [list(cs) for cs in zip(*cssplits)]\n",
    "\n",
    "\n",
    "def call_count(transpose_cssplits: list[list[str]]) -> list[dict[str:int]]:\n",
    "    cssplit_counts = []\n",
    "    for cssplit in transpose_cssplits:\n",
    "        count = Counter(cssplit)\n",
    "        count = dict(count)\n",
    "        cssplit_counts.append(count)\n",
    "    return cssplit_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele=\"control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]\n",
    "# Sample\n",
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "transpose_control = transpose(cssplits_control)\n",
    "transpose_sample = transpose(cssplits_sample)\n",
    "# Make count matrix\n",
    "count_control = call_count(transpose_control)\n",
    "count_sample = call_count(transpose_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=T': 800, 'N': 200}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_control[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=T': 910, 'N': 89}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence errors\n",
    "sequence = FASTA_ALLELES[allele]\n",
    "left_idx, right_idx = set_indexes(sequence)\n",
    "count_5mer_sample = count_indels_5mer(cssplits_sample, left_idx, right_idx)\n",
    "count_5mer_control = count_indels_5mer(cssplits_control, left_idx, right_idx)\n",
    "sequence_errors = extract_sequence_errors(count_5mer_sample, count_5mer_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'sub', 'del'}\n",
      "32 {'ins', 'del'}\n",
      "37 {'sub', 'del'}\n",
      "43 {'sub', 'del'}\n",
      "49 {'sub', 'del'}\n",
      "65 {'sub', 'del'}\n",
      "67 {'sub', 'del'}\n",
      "73 {'sub', 'ins'}\n",
      "95 {'ins', 'del'}\n",
      "96 {'sub', 'del'}\n",
      "108 {'sub', 'del'}\n",
      "120 {'sub', 'del'}\n",
      "125 {'sub', 'del'}\n",
      "130 {'sub', 'del'}\n",
      "165 {'ins'}\n",
      "166 {'ins', 'del'}\n",
      "174 {'sub', 'ins'}\n",
      "188 {'sub', 'del'}\n",
      "205 {'sub', 'ins'}\n",
      "215 {'sub', 'ins'}\n",
      "219 {'sub', 'del'}\n",
      "251 {'sub', 'del'}\n",
      "259 {'sub', 'del'}\n",
      "300 {'sub', 'del'}\n",
      "318 {'ins', 'del'}\n",
      "319 {'sub'}\n",
      "349 {'ins', 'del'}\n",
      "358 {'sub', 'del'}\n",
      "393 {'ins', 'del'}\n",
      "395 {'sub', 'del'}\n",
      "407 {'sub', 'del'}\n",
      "419 {'sub', 'del'}\n",
      "424 {'ins', 'del'}\n",
      "447 {'sub', 'ins'}\n",
      "463 {'ins', 'del'}\n",
      "514 {'sub', 'del'}\n",
      "537 {'sub', 'ins'}\n",
      "538 {'sub', 'del'}\n",
      "542 {'sub', 'del'}\n",
      "548 {'ins', 'del'}\n",
      "566 {'sub', 'del'}\n"
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(sequence_errors):\n",
    "    if len(s) != 3:\n",
    "        print(i, s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "妙に`{'sub', 'del'}`が多い気がします…？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 脱線：`remove_minor_indels`の実装\n",
    "- 5merの中で、indelの数が少ないものを除外する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_errors = [set() for _ in range(len(count_5mer_sample))]\n",
    "dists = defaultdict(list)\n",
    "# Calculate Jensen-Shannon distance\n",
    "for samp, cont in zip(count_5mer_sample, count_5mer_control):\n",
    "    for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "        s = samp[mutation]\n",
    "        c = cont[mutation]\n",
    "        dists[mutation].append(distance.jensenshannon(s, c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(count_5mer_sample[0][\"ins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9, 18, 816, 33]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 828\n",
    "count_5mer_sample[i//5][\"sub\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(cssplits_sample) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_minority_5mer(cssplits, count_5mer: list[dict]) -> list[dict]:\n",
    "    coverage = len(cssplits)\n",
    "    count_5mer_filtered = []\n",
    "    for count in count_5mer:\n",
    "        dict_mutation = defaultdict(list)\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            if all(True for c in count[mutation] if c < coverage*0.01):\n",
    "                count[mutation] = [1] * 5\n",
    "            dict_mutation[mutation] = count[mutation]\n",
    "        count_5mer_filtered.append(dict_mutation)\n",
    "    return count_5mer_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cssplits = deepcopy(cssplits_sample)\n",
    "print(len(cssplits)*0.01)\n",
    "count_5mer = deepcopy(count_5mer_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = remove_minority_5mer(cssplits, count_5mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ins': [1, 1, 1, 1, 1],\n",
       "             'del': [1, 1, 1, 1, 1],\n",
       "             'sub': [1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_minor_indels(cssplits: list[list[str]], count_5mer: list[dict]) -> list[dict]:\n",
    "    coverage = len(cssplits)\n",
    "    count_5mer_filtered = []\n",
    "    for count in count_5mer:\n",
    "        dict_mutation = defaultdict(list)\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            if all(True if c < coverage*0.01 else False for c in count[mutation]):\n",
    "                count[mutation] = [1] * 5\n",
    "            dict_mutation[mutation] = count[mutation]\n",
    "        count_5mer_filtered.append(dict_mutation)\n",
    "    return count_5mer_filtered\n",
    "\n",
    "\n",
    "def count_5mer_indels(cssplits: list[list[str]], left_idx: int, right_idx: int) -> list[dict]:\n",
    "    transposed = [list(t) for t in zip(*cssplits)]\n",
    "    count_5mer = []\n",
    "    for i in range(left_idx, right_idx, 5):\n",
    "        count = {\"ins\": [1] * 5, \"del\": [1] * 5, \"sub\": [1] * 5}\n",
    "        cssplits_5mer = transposed[i : i + 5]\n",
    "        for j, cs in enumerate(cssplits_5mer):\n",
    "            counter = Counter(cs)\n",
    "            for key, cnt in counter.items():\n",
    "                if key.startswith(\"=\") or key == \"N\" or re.search(r\"a|c|g|t|n\", key):\n",
    "                    continue\n",
    "                if key.startswith(\"+\"):\n",
    "                    count[\"ins\"][j] += cnt\n",
    "                elif key.startswith(\"-\"):\n",
    "                    count[\"del\"][j] += cnt\n",
    "                elif key.startswith(\"*\"):\n",
    "                    count[\"sub\"][j] += cnt\n",
    "        count_5mer.append(count)\n",
    "        count_5mer = remove_minor_indels(cssplits, count_5mer)\n",
    "    return count_5mer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence errors\n",
    "sequence = FASTA_ALLELES[allele]\n",
    "left_idx, right_idx = set_indexes(sequence)\n",
    "count_5mer_sample = count_5mer_indels(cssplits_sample, left_idx, right_idx)\n",
    "count_5mer_control = count_5mer_indels(cssplits_control, left_idx, right_idx)\n",
    "sequence_errors = extract_sequence_errors(count_5mer_sample, count_5mer_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 {'sub', 'del'}\n",
      "43 {'sub', 'del'}\n",
      "49 {'sub', 'del'}\n",
      "65 {'sub', 'del'}\n",
      "67 {'sub', 'del'}\n",
      "73 {'sub', 'ins'}\n",
      "95 {'ins', 'del'}\n",
      "96 {'sub', 'del'}\n",
      "108 {'sub', 'del'}\n",
      "125 {'sub', 'del'}\n",
      "165 {'ins'}\n",
      "166 {'ins', 'del'}\n",
      "174 {'sub', 'ins'}\n",
      "188 {'sub', 'del'}\n",
      "205 {'sub', 'ins'}\n",
      "215 {'sub', 'ins'}\n",
      "219 {'sub', 'del'}\n",
      "251 {'sub', 'del'}\n",
      "259 {'sub', 'del'}\n",
      "300 {'sub', 'del'}\n",
      "318 {'ins', 'del'}\n",
      "319 {'sub', 'ins'}\n",
      "349 {'ins', 'del'}\n",
      "358 {'sub', 'del'}\n",
      "393 {'ins', 'del'}\n",
      "395 {'sub', 'del'}\n",
      "407 {'sub', 'del'}\n",
      "419 {'sub', 'del'}\n",
      "424 {'ins', 'del'}\n",
      "447 {'sub', 'ins'}\n",
      "463 {'ins', 'del'}\n",
      "514 {'sub', 'del'}\n",
      "537 {'sub', 'ins'}\n",
      "538 {'sub', 'del'}\n",
      "542 {'sub', 'del'}\n",
      "548 {'ins', 'del'}\n",
      "566 {'sub', 'del'}\n"
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(sequence_errors):\n",
    "    if len(s) != 3:\n",
    "        print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'ins': [1, 1, 1, 1, 1], 'del': [10, 16, 17, 131, 11], 'sub': [7, 9, 18, 816, 33]})\n",
      "defaultdict(<class 'list'>, {'ins': [1, 1, 1, 1, 1], 'del': [4, 11, 15, 30, 22], 'sub': [9, 14, 55, 44, 28]})\n"
     ]
    }
   ],
   "source": [
    "print(count_5mer_sample[165])\n",
    "print(count_5mer_control[165])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 脱線：`remove_minor_indels`が動くかテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変異部位の抽出"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess.correct後のサンプルについてコンセンサス配列を取り、（`N`を無視して）1%以上の変異がある塩基配列部位のみを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barcode31_splice_control.jsonl\n"
     ]
    }
   ],
   "source": [
    "import midsv\n",
    "allele=\"control\"\n",
    "print(f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percent_indels(cssplits):\n",
    "    percelt_indels = []\n",
    "    cssplits_transposed = [list(t) for t in zip(*cssplits)]\n",
    "    for cssplit in cssplits_transposed:\n",
    "        coverage = 0\n",
    "        count_indelsub = {\"ins\":0, \"del\":0, \"sub\":0}\n",
    "        for cs in cssplit:\n",
    "            if cs == \"N\":\n",
    "                continue\n",
    "            coverage += 1\n",
    "            if cs.startswith(\"+\"):\n",
    "                count_indelsub[\"ins\"] += 1\n",
    "            elif cs.startswith(\"-\"):\n",
    "                count_indelsub[\"del\"] += 1\n",
    "            elif cs.startswith(\"*\"):\n",
    "                count_indelsub[\"sub\"] += 1\n",
    "        if coverage == 0:\n",
    "            per_indels = {\"ins\":0, \"del\":0, \"sub\":0}\n",
    "        else:\n",
    "            per_indels = {mutation: (count / coverage * 100) for mutation, count in count_indelsub.items()}\n",
    "        percelt_indels.append(per_indels)\n",
    "    return percelt_indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 {'ins': 1.6227180527383367, 'del': 0.0, 'sub': 0.0}\n",
      "245 {'ins': 1.7258883248730965, 'del': 0.0, 'sub': 0.0}\n",
      "246 {'ins': 1.015228426395939, 'del': 0.0, 'sub': 0.0}\n",
      "325 {'ins': 1.3224821973550356, 'del': 0.0, 'sub': 0.0}\n",
      "335 {'ins': 2.034587995930824, 'del': 0.0, 'sub': 0.0}\n",
      "336 {'ins': 1.119023397761953, 'del': 0.0, 'sub': 0.0}\n",
      "337 {'ins': 1.017293997965412, 'del': 0.0, 'sub': 0.0}\n",
      "365 {'ins': 0.0, 'del': 2.136317395727365, 'sub': 0.0}\n",
      "475 {'ins': 0.0, 'del': 0.0, 'sub': 1.2269938650306749}\n",
      "542 {'ins': 1.3360739979445015, 'del': 0.0, 'sub': 0.0}\n",
      "825 {'ins': 0.0, 'del': 0.9316770186335404, 'sub': 0.6211180124223602}\n",
      "826 {'ins': 0.0, 'del': 1.5527950310559007, 'sub': 0.8281573498964804}\n",
      "827 {'ins': 0.0, 'del': 1.6563146997929608, 'sub': 1.7598343685300208}\n",
      "828 {'ins': 0.0, 'del': 13.457556935817806, 'sub': 84.36853002070393}\n",
      "829 {'ins': 0.0, 'del': 1.0351966873706004, 'sub': 3.3126293995859215}\n",
      "830 {'ins': 0.0, 'del': 0.0, 'sub': 5.383022774327122}\n",
      "872 {'ins': 0.0, 'del': 1.2409513960703205, 'sub': 0.0}\n",
      "874 {'ins': 0.0, 'del': 1.1375387797311272, 'sub': 0.0}\n",
      "1026 {'ins': 0.0, 'del': 1.1458333333333333, 'sub': 0.0}\n",
      "1097 {'ins': 1.1482254697286012, 'del': 0.0, 'sub': 0.0}\n",
      "1502 {'ins': 1.1482254697286012, 'del': 0.0, 'sub': 0.0}\n",
      "1591 {'ins': 0.0, 'del': 0.0, 'sub': 1.1506276150627615}\n",
      "1598 {'ins': 0.41841004184100417, 'del': 1.0460251046025104, 'sub': 0.0}\n",
      "1599 {'ins': 0.7322175732217573, 'del': 0.6276150627615062, 'sub': 0.0}\n",
      "1748 {'ins': 0.0, 'del': 0.0, 'sub': 1.5772870662460567}\n",
      "1749 {'ins': 0.0, 'del': 0.0, 'sub': 1.3669821240799158}\n",
      "1976 {'ins': 1.1591148577449948, 'del': 0.0, 'sub': 0.0}\n",
      "2096 {'ins': 1.0548523206751055, 'del': 0.0, 'sub': 0.0}\n",
      "2122 {'ins': 0.0, 'del': 0.0, 'sub': 1.2658227848101267}\n",
      "2123 {'ins': 0.0, 'del': 0.0, 'sub': 2.9535864978902953}\n",
      "2235 {'ins': 0.0, 'del': 1.689545934530095, 'sub': 0.0}\n",
      "2236 {'ins': 0.0, 'del': 1.2671594508975714, 'sub': 0.0}\n",
      "2237 {'ins': 0.0, 'del': 1.7951425554382259, 'sub': 0.0}\n",
      "2238 {'ins': 0.0, 'del': 1.0559662090813093, 'sub': 0.0}\n",
      "2315 {'ins': 0.0, 'del': 0.0, 'sub': 1.5873015873015872}\n",
      "2573 {'ins': 1.5940488841657812, 'del': 0.0, 'sub': 0.0}\n",
      "2686 {'ins': 0.0, 'del': 3.08839190628328, 'sub': 0.0}\n",
      "2692 {'ins': 1.9169329073482428, 'del': 0.0, 'sub': 0.0}\n",
      "2832 {'ins': 1.4084507042253522, 'del': 0.0, 'sub': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cssplits = cssplits_sample\n",
    "x = calc_percent_indels(cssplits)\n",
    "for i, xx in enumerate(x):\n",
    "    if sum(xx.values()) > 1:\n",
    "        print(i, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 {'ins': 1.0090817356205852, 'del': 0.0, 'sub': 0.0}\n",
      "189 {'ins': 1.6145307769929365, 'del': 0.0, 'sub': 0.0}\n",
      "215 {'ins': 2.330293819655522, 'del': 0.0, 'sub': 0.0}\n",
      "335 {'ins': 1.4285714285714286, 'del': 0.0, 'sub': 0.0}\n",
      "336 {'ins': 1.3265306122448979, 'del': 0.0, 'sub': 0.0}\n",
      "365 {'ins': 0.0, 'del': 2.1450459652706844, 'sub': 0.0}\n",
      "367 {'ins': 0.0, 'del': 1.4300306435137897, 'sub': 0.0}\n",
      "478 {'ins': 0.0, 'del': 0.0, 'sub': 1.5384615384615385}\n",
      "479 {'ins': 0.0, 'del': 0.0, 'sub': 1.4344262295081966}\n",
      "540 {'ins': 1.1293634496919918, 'del': 0.0, 'sub': 0.0}\n",
      "542 {'ins': 2.5667351129363447, 'del': 0.0, 'sub': 0.0}\n",
      "625 {'ins': 1.2371134020618557, 'del': 0.0, 'sub': 0.0}\n",
      "627 {'ins': 1.134020618556701, 'del': 0.0, 'sub': 0.0}\n",
      "825 {'ins': 0.0, 'del': 0.3112033195020747, 'sub': 0.8298755186721992}\n",
      "826 {'ins': 0.0, 'del': 1.0373443983402488, 'sub': 1.3485477178423237}\n",
      "827 {'ins': 0.0, 'del': 1.4522821576763485, 'sub': 5.601659751037345}\n",
      "828 {'ins': 0.0, 'del': 3.008298755186722, 'sub': 4.460580912863071}\n",
      "829 {'ins': 0.0, 'del': 2.1784232365145226, 'sub': 2.8008298755186725}\n",
      "830 {'ins': 0.0, 'del': 0.0, 'sub': 1.3485477178423237}\n",
      "871 {'ins': 0.0, 'del': 1.4522821576763485, 'sub': 0.0}\n",
      "944 {'ins': 1.2461059190031152, 'del': 0.0, 'sub': 0.0}\n",
      "1027 {'ins': 0.0, 'del': 1.0416666666666665, 'sub': 0.0}\n",
      "1028 {'ins': 0.0, 'del': 1.25, 'sub': 0.0}\n",
      "1029 {'ins': 0.0, 'del': 1.5625, 'sub': 0.0}\n",
      "1076 {'ins': 0.0, 'del': 1.251303441084463, 'sub': 0.0}\n",
      "1096 {'ins': 1.044932079414838, 'del': 0.0, 'sub': 0.0}\n",
      "1097 {'ins': 1.8808777429467085, 'del': 0.0, 'sub': 0.0}\n",
      "1257 {'ins': 2.431289640591966, 'del': 0.0, 'sub': 0.0}\n",
      "1297 {'ins': 1.3742071881606766, 'del': 0.0, 'sub': 0.0}\n",
      "1298 {'ins': 1.0570824524312896, 'del': 0.0, 'sub': 0.0}\n",
      "1595 {'ins': 0.7494646680942184, 'del': 0.4282655246252677, 'sub': 0.0}\n",
      "1596 {'ins': 0.5353319057815845, 'del': 0.8565310492505354, 'sub': 0.0}\n",
      "1597 {'ins': 0.32119914346895073, 'del': 0.9635974304068522, 'sub': 0.0}\n",
      "1598 {'ins': 0.5353319057815845, 'del': 0.7494646680942184, 'sub': 0.0}\n",
      "1749 {'ins': 0.0, 'del': 0.0, 'sub': 1.288936627282492}\n",
      "1968 {'ins': 0.0, 'del': 0.0, 'sub': 2.043010752688172}\n",
      "2035 {'ins': 1.6146393972012916, 'del': 0.0, 'sub': 0.0}\n",
      "2096 {'ins': 1.1827956989247312, 'del': 0.0, 'sub': 0.0}\n",
      "2122 {'ins': 0.0, 'del': 0.0, 'sub': 1.3963480128893664}\n",
      "2123 {'ins': 0.0, 'del': 0.0, 'sub': 1.5037593984962405}\n",
      "2235 {'ins': 0.0, 'del': 1.5053763440860215, 'sub': 0.0}\n",
      "2237 {'ins': 0.0, 'del': 1.3978494623655915, 'sub': 0.0}\n",
      "2238 {'ins': 0.0, 'del': 1.6129032258064515, 'sub': 0.0}\n",
      "2239 {'ins': 0.0, 'del': 1.935483870967742, 'sub': 0.0}\n",
      "2315 {'ins': 0.0, 'del': 0.0, 'sub': 1.7222820236813776}\n",
      "2570 {'ins': 1.5217391304347827, 'del': 0.0, 'sub': 0.0}\n",
      "2686 {'ins': 0.0, 'del': 1.6339869281045754, 'sub': 0.0}\n",
      "2688 {'ins': 0.0, 'del': 1.3071895424836601, 'sub': 0.0}\n",
      "2689 {'ins': 0.0, 'del': 1.1982570806100217, 'sub': 0.0}\n",
      "2692 {'ins': 1.3071895424836601, 'del': 0.0, 'sub': 0.0}\n",
      "2693 {'ins': 1.7429193899782136, 'del': 0.0, 'sub': 0.0}\n",
      "2743 {'ins': 0.0, 'del': 0.0, 'sub': 1.5267175572519083}\n",
      "2832 {'ins': 2.4282560706401766, 'del': 0.0, 'sub': 0.0}\n"
     ]
    }
   ],
   "source": [
    "cssplits = cssplits_control\n",
    "x = calc_percent_indels(cssplits)\n",
    "for i, xx in enumerate(x):\n",
    "    if sum(xx.values()) > 1:\n",
    "        print(i, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sample = calc_percent_indels(cssplits_sample)\n",
    "percent_control = calc_percent_indels(cssplits_control)\n",
    "mutation_loci = set()\n",
    "for i, (samp, cont) in enumerate(zip(percent_sample, percent_control)):\n",
    "    for mutation_type in [\"ins\", \"del\", \"sub\"]:\n",
    "        if i in mutation_loci:\n",
    "            break\n",
    "        if abs(samp[mutation_type] - cont[mutation_type]) > 1:\n",
    "            mutation_loci.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ins': 0.4056795131845842, 'del': 0.0, 'sub': 0.0}\n",
      "{'ins': 1.6145307769929365, 'del': 0.0, 'sub': 0.0}\n"
     ]
    }
   ],
   "source": [
    "i=189\n",
    "print(percent_sample[i])\n",
    "print(percent_control[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "def calc_percent_indels(cssplits):\n",
    "    percelt_indels = []\n",
    "    cssplits_transposed = [list(t) for t in zip(*cssplits)]\n",
    "    for cssplit in cssplits_transposed:\n",
    "        coverage = 0\n",
    "        count_indelsub = {\"ins\":0, \"del\":0, \"sub\":0}\n",
    "        for cs in cssplit:\n",
    "            if cs == \"N\":\n",
    "                continue\n",
    "            coverage += 1\n",
    "            if cs.startswith(\"+\"):\n",
    "                count_indelsub[\"ins\"] += 1\n",
    "            elif cs.startswith(\"-\"):\n",
    "                count_indelsub[\"del\"] += 1\n",
    "            elif cs.startswith(\"*\"):\n",
    "                count_indelsub[\"sub\"] += 1\n",
    "        if coverage == 0:\n",
    "            per_indels = {\"ins\":0, \"del\":0, \"sub\":0}\n",
    "        else:\n",
    "            per_indels = {mutation: (count / coverage * 100) for mutation, count in count_indelsub.items()}\n",
    "        percelt_indels.append(per_indels)\n",
    "    return percelt_indels\n",
    "\n",
    "def extract_mutation_loci(cssplits_sample, cssplits_control) -> set():\n",
    "    percent_sample = calc_percent_indels(cssplits_sample)\n",
    "    percent_control = calc_percent_indels(cssplits_control)\n",
    "    mutation_loci = set()\n",
    "    for i, (samp, cont) in enumerate(zip(percent_sample, percent_control)):\n",
    "        for mutation_type in [\"ins\", \"del\", \"sub\"]:\n",
    "            if i in mutation_loci:\n",
    "                break\n",
    "            if abs(samp[mutation_type] - cont[mutation_type]) > 1:\n",
    "                mutation_loci.add(i)\n",
    "    return mutation_loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ins': 0.4056795131845842, 'del': 0.0, 'sub': 0.0}\n",
      "{'ins': 1.6145307769929365, 'del': 0.0, 'sub': 0.0}\n"
     ]
    }
   ],
   "source": [
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]\n",
    "\n",
    "i=189\n",
    "print(percent_sample[i])\n",
    "print(percent_control[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_sequence(sequence: str):\n",
    "    list_seq = list(sequence)\n",
    "    list_seq[0] = \"X\"\n",
    "    return \"\".join(list_seq)\n",
    "\n",
    "\n",
    "def edit_list_sequence(list_sequence: list):\n",
    "    list_sequence_edit = list_sequence.copy()\n",
    "    list_sequence_edit[0] = \"X\"\n",
    "    return list_sequence_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAAAA\n",
      "['AAA', 'AAA', 'AAA']\n"
     ]
    }
   ],
   "source": [
    "sequence = [\"A\" * 5]\n",
    "list_sequence = list(sequence)\n",
    "\n",
    "x = edit_sequence(sequence)\n",
    "print(x[:5])\n",
    "print(sequence[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAA', 'AAA', 'AAA']\n",
      "['X', 'AAA', 'AAA']\n",
      "['AAA', 'AAA', 'AAA']\n"
     ]
    }
   ],
   "source": [
    "x = edit_list_sequence(list_sequence)\n",
    "print(x[:5])\n",
    "print(list_sequence[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 ms ± 3.83 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 2D list\n",
    "list_sequence = [[\".\"] * 100 for _ in range(100)]\n",
    "# Deep copy\n",
    "list_sequence_edit = deepcopy(list_sequence)\n",
    "# Edit\n",
    "list_sequence_edit[0][0] = \"@\"\n",
    "# print(list_sequence)\n",
    "# print(list_sequence_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.1 µs ± 454 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 1D list\n",
    "sequence = [\".\" * 100 for _ in range(100)]\n",
    "# Shallow copy\n",
    "sequence_edit = sequence.copy()\n",
    "# Split to 2D list\n",
    "sequence_edit = [list(s) for s in sequence_edit]\n",
    "# Edit\n",
    "sequence_edit[0][0] = \"@\"\n",
    "# Revert to 1D list\n",
    "sequence_edit = [\"\".join(s) for s in sequence_edit]\n",
    "# print(sequence)\n",
    "# print(sequence_edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deepcopyは遅い\n",
    "- 二次元配列を操作する際には、リストのコピーを作成することで、元のリストを変更しないようにしたい\n",
    "- 一方でdeepcopyは遅く、copyはshallow copyなので、コピーしたリストを変更するともとのリストまで変更される\n",
    "- なのでもとのオブジェクトが文字列ならばオブジェクトは1次元の文字列で持っておき、関数の中でその都度リストに変換して操作するとオリジナルの文字列は変更されず、かつこちらのほうがdeepcopyを使うよりも遥かに高速"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `preprocess.extract_mutation_locy`を`quickstart.py`に組み込んだ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に取り組むこと"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "+ [ ] mutation_lociを利用したアレルの分類・クラスタリング手法を考える\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "+ [ ] Insertionのなかにある変異を同定する手法を考案する\n",
    "+ [ ] Ayabe-taks1のright_loxpがいまいちな理由を考察する\n",
    "+ [ ] `preprocess.correct_sequence_error.replace_atmark`のコードがわかりにくい\n",
    "    + テストを用意してリファクタリングする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか\n",
    "\n",
    "両者については、`correct_sequence_error`などで補正するときに、**変異候補の塩基配列のみを対象とする**ことで対応できる可能性がある\n",
    "\n",
    "- 変異候補の塩基配列のみを対象とする\n",
    "    - 両端が欠失しているようなリードについて、変異候補部位を含まないリードは`uncategorized`といったカテゴリにできる\n",
    "    - よって**変異候補部位を含むか含まないか**を考えることで、短いリードや両端が欠失しているリードの分類が可能になる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a09285d6cbc768c1977f96e8deb5ca1ec0d08675e9573ed6dfd37fd7d91de663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
