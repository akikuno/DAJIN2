{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取り組み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy.spatialの`distance.cosine`を除いてみたところ、すでに1-cosineをされていたことに気がついた。  \n",
    "現状は`1-distance.cosine`をしており、これでは距離ではなくて類似度を求めている。  \n",
    "適切な距離を定義したい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Research/DAJIN2\n"
     ]
    }
   ],
   "source": [
    "# ルートディレクトリをPathに含めるおまじない\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "if os.getcwd() != \"/mnt/d/Research/DAJIN2\":\n",
    "    parent_path = str(Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.parent)\n",
    "    sys.path.append(parent_path)\n",
    "    os.chdir(parent_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = [1,1,100,1,1]\n",
    "cont = [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5349908532110781\n"
     ]
    }
   ],
   "source": [
    "print(distance.cosine(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(distance.correlation(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n"
     ]
    }
   ],
   "source": [
    "print(distance.euclidean(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = [1,1,2,1,1]\n",
    "cont = [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(distance.euclidean(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.788854381999832\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "samp = [0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "cont = [0.1 ,0.1 ,0.1 ,0.1 ,0.1]\n",
    "print(distance.euclidean(samp, cont))\n",
    "print(distance.cosine(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.3694073749055342\n"
     ]
    }
   ],
   "source": [
    "samp = [0.1, 0.1, 0.9, 0.1, 0.1]\n",
    "cont = [0.1 ,0.1 ,0.1 ,0.1 ,0.1]\n",
    "print(distance.euclidean(samp, cont))\n",
    "print(distance.cosine(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.788854381999832\n"
     ]
    }
   ],
   "source": [
    "samp = [0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "cont = [0.1 ,0.1 ,0.1 ,0.1 ,0.1 ]\n",
    "print(distance.euclidean(samp, cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルビノ点変異(1%)のwindow countingで試行錯誤する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test-tyr-albino-01%...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"/mnt/d/Research/DAJIN2\")\n",
    "os.chdir(\"/mnt/d/Research/DAJIN2\")\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "from src.DAJIN2.core import preprocess, classification, clustering, consensus, report\n",
    "from src.DAJIN2.core.clustering import clustering\n",
    "\n",
    "##### # * Subset of Point mutation\n",
    "##### # 50 or 10 or 01%\n",
    "percent = \"01\"\n",
    "SAMPLE, CONTROL, ALLELE, NAME, GENOME, DEBUG, THREADS = (\n",
    "    f\"misc/data/tyr_albino_{percent}%.fq.gz\",\n",
    "    \"misc/data/tyr_control.fq.gz\",\n",
    "    \"misc/data/tyr_control.fasta\",\n",
    "    f\"test-tyr-albino-{percent}%\",\n",
    "    \"mm10\",\n",
    "    True,\n",
    "    14,\n",
    ")\n",
    "print(f\"processing {NAME}...\")\n",
    "\n",
    "##########################################################\n",
    "# Check inputs\n",
    "##########################################################\n",
    "preprocess.check_inputs.check_files(SAMPLE, CONTROL, ALLELE)\n",
    "TEMPDIR = Path(\"DAJINResults\", \".tempdir\", NAME)\n",
    "IS_CACHE_CONTROL = preprocess.check_inputs.exists_cached_control(CONTROL, TEMPDIR)\n",
    "IS_CACHE_GENOME = preprocess.check_inputs.exists_cached_genome(GENOME, TEMPDIR, IS_CACHE_CONTROL)\n",
    "UCSC_URL, GOLDENPATH_URL = None, None\n",
    "if GENOME and not IS_CACHE_GENOME:\n",
    "    UCSC_URL, GOLDENPATH_URL = preprocess.check_inputs.check_and_fetch_genome(GENOME)\n",
    "\n",
    "##########################################################\n",
    "# Format inputs\n",
    "##########################################################\n",
    "SAMPLE_NAME = preprocess.format_inputs.extract_basename(SAMPLE)\n",
    "CONTROL_NAME = preprocess.format_inputs.extract_basename(CONTROL)\n",
    "FASTA_ALLELES = preprocess.format_inputs.dictionize_allele(ALLELE)\n",
    "\n",
    "preprocess.format_inputs.make_directories(TEMPDIR, SAMPLE_NAME, CONTROL_NAME)\n",
    "\n",
    "if GENOME:\n",
    "    GENOME_COODINATES = preprocess.format_inputs.fetch_coodinate(GENOME, UCSC_URL, FASTA_ALLELES[\"control\"])\n",
    "    CHROME_SIZE = preprocess.format_inputs.fetch_chrom_size(GENOME_COODINATES[\"chr\"], GENOME, GOLDENPATH_URL)\n",
    "    preprocess.format_inputs.cache_coodinates_and_chromsize(TEMPDIR, GENOME, GENOME_COODINATES, CHROME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Export fasta files as single-FASTA format\n",
    "################################################################################\n",
    "# TODO: use yeild, not export\n",
    "for identifier, sequence in FASTA_ALLELES.items():\n",
    "    contents = \"\\n\".join([\">\" + identifier, sequence]) + \"\\n\"\n",
    "    output_fasta = Path(TEMPDIR, \"fasta\", f\"{identifier}.fasta\")\n",
    "    output_fasta.write_text(contents)\n",
    "###############################################################################\n",
    "# Mapping with mappy\n",
    "###############################################################################\n",
    "for path_fasta in Path(TEMPDIR, \"fasta\").glob(\"*.fasta\"):\n",
    "    name_fasta = path_fasta.stem\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, threads=THREADS)\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, CONTROL, CONTROL_NAME, preset=\"splice\", threads=THREADS\n",
    "    )\n",
    "    preprocess.mappy_align.output_sam(\n",
    "        TEMPDIR, path_fasta, name_fasta, SAMPLE, SAMPLE_NAME, preset=\"splice\", threads=THREADS\n",
    "    )\n",
    "########################################################################\n",
    "# MIDSV conversion\n",
    "########################################################################\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{CONTROL_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n",
    "for path_sam in Path(TEMPDIR, \"sam\").glob(f\"{SAMPLE_NAME}_splice_*\"):\n",
    "    preprocess.calc_midsv.output_midsv(TEMPDIR, path_sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Correct CSSPLITS\n",
    "###############################################################################\n",
    "preprocess.correct_revititive_deletions.execute(TEMPDIR, FASTA_ALLELES, CONTROL_NAME, SAMPLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct_sequence_error.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import midsv\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def set_indexes(sequence: str):\n",
    "    sequence_length = len(sequence)\n",
    "    num_subset = sequence_length % 5\n",
    "    left_idx = 0\n",
    "    right_idx = sequence_length\n",
    "    if num_subset == 1:\n",
    "        left_idx += 1\n",
    "    elif num_subset == 2:\n",
    "        left_idx += 1\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 3:\n",
    "        left_idx += 2\n",
    "        right_idx -= 1\n",
    "    elif num_subset == 4:\n",
    "        left_idx += 2\n",
    "        right_idx -= 2\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "def count_indels_5mer(cssplits: list[list[str]], left_idx: int, right_idx: int) -> list[dict]:\n",
    "    transposed = [list(t) for t in zip(*cssplits)]\n",
    "    count_indels_5mer = []\n",
    "    for i in range(left_idx, right_idx, 5):\n",
    "        count = {\"ins\": [1] * 5, \"del\": [1] * 5, \"sub\": [1] * 5}\n",
    "        cssplits_5mer = transposed[i : i + 5]\n",
    "        for j, cs in enumerate(cssplits_5mer):\n",
    "            counter = Counter(cs)\n",
    "            for key, cnt in counter.items():\n",
    "                if key.startswith(\"=\") or key == \"N\" or re.search(r\"a|c|g|t|n\", key):\n",
    "                    continue\n",
    "                if key.startswith(\"+\"):\n",
    "                    count[\"ins\"][j] += cnt\n",
    "                elif key.startswith(\"-\"):\n",
    "                    count[\"del\"][j] += cnt\n",
    "                elif key.startswith(\"*\"):\n",
    "                    count[\"sub\"][j] += cnt\n",
    "        count_indels_5mer.append(count)\n",
    "    return count_indels_5mer\n",
    "\n",
    "\n",
    "def extract_sequence_errors(count_5mer_sample, count_5mer_control, coverage_sample, coverage_control):\n",
    "    sequence_errors = [set() for _ in range(len(count_5mer_sample))]\n",
    "    for i in range(len(sequence_errors)):\n",
    "        for mutation in [\"ins\", \"del\", \"sub\"]:\n",
    "            samp = [c / coverage_sample for c in count_5mer_sample[i][mutation]]\n",
    "            cont = [c / coverage_control for c in count_5mer_control[i][mutation]]\n",
    "            dist = 1 - distance.cosine(samp, cont)\n",
    "            _, pvalue = stats.ttest_ind(samp, cont, equal_var=False)\n",
    "            if dist > 0.9 and pvalue > 0.05:\n",
    "                sequence_errors[i].add(mutation)\n",
    "    return sequence_errors\n",
    "\n",
    "\n",
    "def replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx):\n",
    "    cssplits_replaced = []\n",
    "    for samp in cssplits_sample:\n",
    "        samp_replaced = deepcopy(samp)\n",
    "        for idx_error, idx_5mer in enumerate(range(left_idx, right_idx, 5)):\n",
    "            samp_5mer = samp[idx_5mer : idx_5mer + 5]\n",
    "            error = sequence_errors[idx_error]\n",
    "            if \"ins\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"+\") else cs for cs in samp_5mer]\n",
    "            if \"del\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"-\") else cs for cs in samp_5mer]\n",
    "            if \"sub\" in error:\n",
    "                samp_5mer = [\"@\" if cs.startswith(\"*\") else cs for cs in samp_5mer]\n",
    "            samp_replaced[idx_5mer : idx_5mer + 5] = samp_5mer\n",
    "        cssplits_replaced.append(samp_replaced)\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "def replace_atmark(cssplits: list[list[str]], sequence: str) -> list[list[str]]:\n",
    "    random.seed(1)\n",
    "    cssplits_replaced = deepcopy(cssplits)\n",
    "    sequence_length = len(sequence)\n",
    "    for i in range(1, sequence_length - 1):\n",
    "        cssplits_atmark = defaultdict(str)\n",
    "        cssplits_sampling_key = defaultdict(list)\n",
    "        cssplits_sampling_all = []\n",
    "        flag_all_atmark = True\n",
    "        for idx, cssplit in enumerate(cssplits):\n",
    "            key = \",\".join([cssplit[i - 1], cssplit[i + 1]])\n",
    "            if cssplit[i] == \"@\":\n",
    "                cssplits_atmark[idx] = key\n",
    "            else:\n",
    "                cssplits_sampling_key[key].append(cssplit[i])\n",
    "                cssplits_sampling_all.append(cssplit[i])\n",
    "                flag_all_atmark = False\n",
    "        for idx, key in cssplits_atmark.items():\n",
    "            if flag_all_atmark:\n",
    "                cssplits_replaced[idx][i] = \"N\"\n",
    "            elif cssplits_sampling_key[key]:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_key[key])\n",
    "            else:\n",
    "                cssplits_replaced[idx][i] = random.choice(cssplits_sampling_all)\n",
    "    for cs in cssplits_replaced:\n",
    "        if cs[0] == \"@\":\n",
    "            cs[0] = \"=\" + sequence[0]\n",
    "        if cs[-1] == \"@\":\n",
    "            cs[-1] = \"=\" + sequence[-1]\n",
    "    return cssplits_replaced\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# main\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def execute(TEMPDIR: Path, FASTA_ALLELES: dict[str, str], CONTROL_NAME: str, SAMPLE_NAME: str) -> None:\n",
    "    for allele, sequence in FASTA_ALLELES.items():\n",
    "        midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "        midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "        coverage_sample = len(midsv_sample)\n",
    "        coverage_control = len(midsv_control)\n",
    "        cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "        cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]\n",
    "        left_idx, right_idx = set_indexes(sequence)\n",
    "        count_5mer_sample = count_indels_5mer(cssplits_sample, left_idx, right_idx)\n",
    "        count_5mer_control = count_indels_5mer(cssplits_control, left_idx, right_idx)\n",
    "        sequence_errors = extract_sequence_errors(\n",
    "            count_5mer_sample, count_5mer_control, coverage_sample, coverage_control\n",
    "        )\n",
    "        cssplits_sample_error_replaced = replace_errors_to_atmark(cssplits_sample, sequence_errors, left_idx, right_idx)\n",
    "        cssplits_control_error_replaced = replace_errors_to_atmark(\n",
    "            cssplits_control, sequence_errors, left_idx, right_idx\n",
    "        )\n",
    "        cssplits_sample_atmark_replaced = replace_atmark(cssplits_sample_error_replaced, sequence)\n",
    "        cssplits_control_atmark_replaced = replace_atmark(cssplits_control_error_replaced, sequence)\n",
    "        # Replace CSSPLIT\n",
    "        cssplits_sample_corrected = [\",\".join(cs) for cs in cssplits_sample_atmark_replaced]\n",
    "        cssplits_control_corrected = [\",\".join(cs) for cs in cssplits_control_atmark_replaced]\n",
    "        for i, cssplits in enumerate(cssplits_sample_corrected):\n",
    "            midsv_sample[i][\"CSSPLIT\"] = cssplits\n",
    "        for i, cssplits in enumerate(cssplits_control_corrected):\n",
    "            midsv_control[i][\"CSSPLIT\"] = cssplits\n",
    "        midsv.write_jsonl(midsv_control, Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\"))\n",
    "        midsv.write_jsonl(midsv_sample, Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele = \"control\"\n",
    "sequence = FASTA_ALLELES[allele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "midsv_sample = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{SAMPLE_NAME}_splice_{allele}.jsonl\")))\n",
    "midsv_control = midsv.read_jsonl((Path(TEMPDIR, \"midsv\", f\"{CONTROL_NAME}_splice_{allele}.jsonl\")))\n",
    "coverage_sample = len(midsv_sample)\n",
    "coverage_control = len(midsv_control)\n",
    "cssplits_sample = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_sample]\n",
    "cssplits_control = [cs[\"CSSPLIT\"].split(\",\") for cs in midsv_control]\n",
    "left_idx, right_idx = set_indexes(sequence)\n",
    "count_5mer_sample = count_indels_5mer(cssplits_sample, left_idx, right_idx)\n",
    "count_5mer_control = count_indels_5mer(cssplits_control, left_idx, right_idx)\n",
    "sequence_errors = extract_sequence_errors(\n",
    "    count_5mer_sample, count_5mer_control, coverage_sample, coverage_control\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'=G': 9116, 'N': 3, '*GT': 80, '=c': 1, '-G': 1, '+A|=G': 1})\n",
      "defaultdict(<class 'int'>, {'=G': 9208, 'N': 3, '=c': 1, '-G': 1, '+A|=G': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "count = defaultdict(int)\n",
    "for cs in cssplits_sample:\n",
    "    count[cs[828]] += 1\n",
    "print(count)\n",
    "\n",
    "count = defaultdict(int)\n",
    "for cs in cssplits_control:\n",
    "    count[cs[828]] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ins': [3, 1, 1, 2, 2], 'del': [1, 1, 2, 2, 1], 'sub': [1, 1, 1, 81, 1]}\n",
      "{'ins': [3, 1, 1, 2, 2], 'del': [1, 1, 2, 2, 1], 'sub': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "i_pm = 828\n",
    "print(count_5mer_sample[i_pm//5])\n",
    "print(count_5mer_control[i_pm//5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n",
      "0.4691552259617494\n"
     ]
    }
   ],
   "source": [
    "samp = [1, 1, 1, 81, 1]\n",
    "cont = [1,1,1,1,1]\n",
    "print(distance.euclidean(samp, cont))\n",
    "print(1 - distance.cosine(samp, cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.0\n",
      "0.9373611373027213\n"
     ]
    }
   ],
   "source": [
    "samp = [1, 1, 1, 81, 1]\n",
    "cont = [1,1,1,5,1]\n",
    "print(distance.euclidean(samp, cont))\n",
    "print(1 - distance.cosine(samp, cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine類似度だけだとControlがほんの少しの変化しただけで大きく類似度が変化してしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [x] Albino点変異の1%\n",
    "\n",
    "無事に1%のアレルを補足できていることがわかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次回の課題"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [x] ~~点変異、欠失、ノックインのデザインで動くかを確認する~~\n",
    "+ [x] ~~Clusteringのときの警告メッセージは無害なので消去する~~\n",
    "+ [ ] right_loxpがいまいちな理由を考察する\n",
    "    +  ~~[x] Ayabe-task1のpreprocessにおいて、329と1280の補正がされていない理由を検討する~~\n",
    "+ [x] ~~distanceの閾値を下げた状態で、1%点変異が検出できるか~~\n",
    "+ [ ] Insertionのなかにある変異を同定する手法を考案する\n",
    "+ [ ] `preprocess.correct_sequence_error.replace_atmark`のコードがわかりにくい\n",
    "    + テストを用意してリファクタリングする\n",
    "+ [ ] cis変異の両端が欠失している場合に、Nで置き換えるとtransとなってしまうのをどうするか（`replace_n`）\n",
    "+ [ ] 短いリードの扱いをどうするべきか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a09285d6cbc768c1977f96e8deb5ca1ec0d08675e9573ed6dfd37fd7d91de663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
